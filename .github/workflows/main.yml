name: Complete CI/CD Pipeline - Deploy & Destroy

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      skip_destroy:
        description: "Skip infrastructure destroy at the end"
        required: false
        type: boolean
        default: false
      deployment_mode:
        description: "Deployment mode"
        required: false
        type: choice
        default: "full"
        options:
          - full
          - infrastructure-only
          - configuration-only

permissions:
  contents: read
  packages: write
  id-token: write

env:
  PYTHON_VERSION: "3.11"
  TERRAFORM_VERSION: "1.6.0"
  ANSIBLE_VERSION: "8.7.0"
  AWS_REGION: "us-east-1"
  PROJECT_NAME: "devops-mid"

jobs:
  # ============================================
  # STAGE 1: Build & Test Application
  # ============================================
  build-and-test:
    name: "ðŸ“¦ Stage 1: Build & Test"
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run unit tests
        run: |
          pytest tests/ -v --cov=app --cov-report=xml --cov-report=term
        continue-on-error: true

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results
          path: |
            coverage.xml
            pytest-results.xml
          retention-days: 7

      - name: Test database connection
        run: |
          python test_db_connection.py || echo "Database test skipped (no DB yet)"
        continue-on-error: true

  # ============================================
  # STAGE 2: Security Scanning & Linting
  # ============================================
  security-and-lint:
    name: "ðŸ”’ Stage 2: Security & Linting"
    runs-on: ubuntu-latest
    needs: build-and-test

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install linting tools
        run: |
          pip install flake8 pylint bandit safety black isort

      - name: Run Flake8
        run: |
          flake8 app/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 app/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
        continue-on-error: true

      - name: Run Pylint
        run: |
          pylint app/ --exit-zero
        continue-on-error: true

      - name: Run Bandit (Security Scanner)
        run: |
          bandit -r app/ -f json -o bandit-report.json || true
          bandit -r app/ -ll
        continue-on-error: true

      - name: Check Python dependencies for vulnerabilities
        run: |
          safety check --json || true
        continue-on-error: true

      - name: Run Black (Code Formatter Check)
        run: |
          black --check app/ || true
        continue-on-error: true

      - name: Run isort (Import Sorter Check)
        run: |
          isort --check-only app/ || true
        continue-on-error: true

      - name: Terraform Security Scan (tfsec)
        uses: aquasecurity/tfsec-action@v1.0.3
        with:
          working_directory: infra
          soft_fail: true

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
          retention-days: 30

  # ============================================
  # STAGE 3: Docker Build & Push
  # ============================================
  docker-build-push:
    name: "ðŸ³ Stage 3: Docker Build & Push"
    runs-on: ubuntu-latest
    needs: security-and-lint
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Log in to GitHub Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: |
            ${{ secrets.DOCKERHUB_USERNAME }}/devops-mid
            ghcr.io/${{ github.repository }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ secrets.DOCKERHUB_USERNAME }}/devops-mid:latest
          format: "sarif"
          output: "trivy-results.sarif"
        continue-on-error: true

      - name: Upload Trivy results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: "trivy-results.sarif"
        continue-on-error: true

  # ============================================
  # STAGE 4: Terraform Infrastructure Provisioning
  # ============================================
  terraform-provision:
    name: "ðŸ—ï¸ Stage 4: Terraform Provision"
    runs-on: ubuntu-latest
    needs: docker-build-push
    if: github.event_name != 'pull_request'
    defaults:
      run:
        working-directory: infra
    outputs:
      ec2_ips: ${{ steps.output.outputs.ec2_ips }}
      rds_endpoint: ${{ steps.output.outputs.rds_endpoint }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      - name: Terraform Format Check
        run: terraform fmt -check -recursive
        continue-on-error: true

      - name: Terraform Init
        run: terraform init

      - name: Terraform Validate
        run: terraform validate

      - name: Clean Up Conflicting Resources
        run: |
          # Configure AWS CLI
          export AWS_DEFAULT_REGION=${{ env.AWS_REGION }}

          # ==== CLEAN UP OLD VPCs (to avoid VpcLimitExceeded) ====
          echo "Checking for old project VPCs..."
          for vpc in $(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" --query "Vpcs[*].VpcId" --output text 2>/dev/null); do
            echo "Found VPC: $vpc - cleaning up dependencies..."
            
            # Delete EC2 instances in VPC
            for instance in $(aws ec2 describe-instances --filters "Name=vpc-id,Values=$vpc" "Name=instance-state-name,Values=running,stopped" --query "Reservations[*].Instances[*].InstanceId" --output text 2>/dev/null); do
              echo "Terminating EC2 instance: $instance"
              aws ec2 terminate-instances --instance-ids "$instance" 2>/dev/null || true
            done
            
            # Wait for instances to terminate
            sleep 30
            
            # Delete NAT Gateways
            for nat in $(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$vpc" "Name=state,Values=available" --query "NatGateways[*].NatGatewayId" --output text 2>/dev/null); do
              echo "Deleting NAT Gateway: $nat"
              aws ec2 delete-nat-gateway --nat-gateway-id "$nat" 2>/dev/null || true
            done
            
            # Delete Internet Gateways
            for igw in $(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$vpc" --query "InternetGateways[*].InternetGatewayId" --output text 2>/dev/null); do
              echo "Detaching and deleting Internet Gateway: $igw"
              aws ec2 detach-internet-gateway --internet-gateway-id "$igw" --vpc-id "$vpc" 2>/dev/null || true
              aws ec2 delete-internet-gateway --internet-gateway-id "$igw" 2>/dev/null || true
            done
            
            # Delete Subnets
            for subnet in $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc" --query "Subnets[*].SubnetId" --output text 2>/dev/null); do
              echo "Deleting Subnet: $subnet"
              aws ec2 delete-subnet --subnet-id "$subnet" 2>/dev/null || true
            done
            
            # Delete Route Tables (non-main)
            for rt in $(aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$vpc" --query "RouteTables[?Associations[0].Main!=\`true\`].RouteTableId" --output text 2>/dev/null); do
              echo "Deleting Route Table: $rt"
              aws ec2 delete-route-table --route-table-id "$rt" 2>/dev/null || true
            done
            
            # Delete Security Groups (non-default)
            for sg in $(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$vpc" --query "SecurityGroups[?GroupName!=\`default\`].GroupId" --output text 2>/dev/null); do
              echo "Deleting Security Group: $sg"
              aws ec2 delete-security-group --group-id "$sg" 2>/dev/null || true
            done
            
            # Finally delete the VPC
            echo "Deleting VPC: $vpc"
            aws ec2 delete-vpc --vpc-id "$vpc" 2>/dev/null || echo "Could not delete VPC $vpc - may have remaining dependencies"
          done

          # ==== Delete DB subnet groups ====
          for sg in $(aws rds describe-db-subnet-groups --query "DBSubnetGroups[?starts_with(DBSubnetGroupName, '${{ env.PROJECT_NAME }}-db-subnet')].DBSubnetGroupName" --output text 2>/dev/null); do
            echo "Deleting DB subnet group: $sg"
            aws rds delete-db-subnet-group --db-subnet-group-name "$sg" 2>/dev/null || echo "Could not delete $sg"
          done

          # ==== Delete orphaned DB instances ====
          for db in $(aws rds describe-db-instances --query "DBInstances[?starts_with(DBInstanceIdentifier, '${{ env.PROJECT_NAME }}-postgres')].DBInstanceIdentifier" --output text 2>/dev/null); do
            echo "Found DB instance: $db - checking status..."
            status=$(aws rds describe-db-instances --db-instance-identifier "$db" --query "DBInstances[0].DBInstanceStatus" --output text 2>/dev/null)
            if [ "$status" != "deleting" ]; then
              echo "Deleting DB instance: $db"
              aws rds delete-db-instance --db-instance-identifier "$db" --skip-final-snapshot --delete-automated-backups 2>/dev/null || echo "Could not delete $db"
            fi
          done

          echo "Waiting for resource cleanup..."
          sleep 15
        continue-on-error: true

      - name: Import Existing Resources (if any)
        run: |
          set -euo pipefail

          # Import existing key pair if present. Some provider import implementations may try
          # to call ImportKeyPair which fails if the key already exists; to avoid that,
          # temporarily comment out the public_key line in the resource before importing.
          if aws ec2 describe-key-pairs --query "KeyPairs[?KeyName=='${{ env.PROJECT_NAME }}-key'].KeyName" --output text 2>/dev/null | grep -q .; then
            echo "Existing key pair '${{ env.PROJECT_NAME }}-key' detected. Preparing to import into Terraform state."
            EC2_TF=ec2.tf
            BACKUP=${EC2_TF}.bak
            if [ -f "$EC2_TF" ]; then
              cp "$EC2_TF" "$BACKUP"
              # Comment out the public_key line to avoid provider trying to import key material
              sed -E "s/^(\s*public_key\s*=\s*.*)/# \1/" "$BACKUP" > "$EC2_TF"
            fi
            terraform import aws_key_pair.deployer "${{ env.PROJECT_NAME }}-key" || echo "Import failed or already imported"
            # Restore original file if we modified it
            if [ -f "$BACKUP" ]; then
              mv "$BACKUP" "$EC2_TF"
            fi
          else
            echo "No existing key pair named '${{ env.PROJECT_NAME }}-key'"
          fi

          # Detect and import any existing DB subnet groups that match the project prefix
          DB_SUBNETS=$(aws rds describe-db-subnet-groups --query "DBSubnetGroups[?starts_with(DBSubnetGroupName, '${{ env.PROJECT_NAME }}-db-subnet')].DBSubnetGroupName" --output text 2>/dev/null || true)
          if [ -n "$DB_SUBNETS" ]; then
            for sg in $DB_SUBNETS; do
              echo "Found existing DB subnet group: $sg - importing into Terraform state as aws_db_subnet_group.main"
              terraform import aws_db_subnet_group.main "$sg" || echo "Failed to import $sg"
              break
            done
          else
            echo "No existing DB subnet group found with prefix '${{ env.PROJECT_NAME }}-db-subnet'"
          fi

          # Detect and import existing RDS DB instances matching the project prefix
          DB_INSTANCES=$(aws rds describe-db-instances --query "DBInstances[?starts_with(DBInstanceIdentifier, '${{ env.PROJECT_NAME }}-postgres')].DBInstanceIdentifier" --output text 2>/dev/null || true)
          if [ -n "$DB_INSTANCES" ]; then
            for db in $DB_INSTANCES; do
              echo "Found existing RDS instance: $db - importing into Terraform state as aws_db_instance.postgres"
              terraform import aws_db_instance.postgres "$db" || echo "Failed to import $db"
              break
            done
          else
            echo "No existing RDS instances found with prefix '${{ env.PROJECT_NAME }}-postgres'"
          fi
        continue-on-error: true

      - name: Terraform Plan
        run: |
          terraform plan \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="docker_image=${{ secrets.DOCKERHUB_USERNAME }}/devops-mid:latest" \
            -var="ec2_instance_type=t3.micro" \
            -var="ec2_instance_count=1" \
            -var="manage_ssh_key=false" \
            -var="db_instance_class=db.t3.micro" \
            -out=tfplan

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan

      - name: Save Terraform State
        uses: actions/upload-artifact@v4
        with:
          name: terraform-state
          path: infra/terraform.tfstate
          retention-days: 7

      - name: Extract Terraform Outputs
        id: output
        run: |
          set -euo pipefail

          # Safely capture ec2_public_ips as JSON array or default to empty array
          EC2_JSON=$(terraform output -json ec2_public_ips 2>/dev/null || echo '[]')
          # Normalize Terraform warning text if present
          if echo "$EC2_JSON" | grep -qi "no outputs"; then
            EC2_JSON='[]'
          fi
          echo "ec2_ips=$EC2_JSON" >> $GITHUB_OUTPUT

          # Capture rds endpoint or empty string if not present
          RDS_EP=$(terraform output -raw rds_endpoint 2>/dev/null || echo '')
          if echo "$RDS_EP" | grep -qi "no outputs"; then
            RDS_EP=''
          fi
          echo "rds_endpoint=$RDS_EP" >> $GITHUB_OUTPUT

          # Persist human-readable terraform output but do not fail if absent
          mkdir -p ${{ github.workspace }}/outputs
          terraform output > ${{ github.workspace }}/outputs/terraform-output.txt 2>/dev/null || true

      - name: Upload Terraform Outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs
          path: ${{ github.workspace }}/outputs/terraform-output.txt
          retention-days: 30

      - name: Wait for EC2 to be ready
        run: sleep 60

  # ============================================
  # STAGE 5: Ansible Configuration & Deployment
  # ============================================
  ansible-deploy:
    name: "âš™ï¸ Stage 5: Ansible Deploy"
    runs-on: ubuntu-latest
    needs: terraform-provision
    if: github.event_name != 'pull_request'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Ansible and dependencies
        run: |
          pip install ansible==${{ env.ANSIBLE_VERSION }}
          pip install boto3 botocore
          cd ansible
          ansible-galaxy collection install -r requirements.yml

      - name: Configure AWS credentials for dynamic inventory
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ needs.terraform-provision.outputs.ec2_ips }} >> ~/.ssh/known_hosts || true

      - name: Setup Terraform for inventory generation
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      - name: Install jq
        run: |
          sudo apt-get update && sudo apt-get install -y jq

      - name: Generate Ansible Inventory
        run: |
          cd infra
          terraform init
          
          # Check if outputs exist
          if ! terraform output -json ec2_public_ips > /dev/null 2>&1; then
            echo "âš ï¸  Terraform outputs not found. Skipping inventory generation."
            echo "   Run 'terraform apply' first to create infrastructure."
            exit 0
          fi
          
          # Get Terraform outputs with error handling
          EC2_IPS=$(terraform output -json ec2_public_ips 2>/dev/null | jq -r '.[]' 2>/dev/null || echo "")
          RDS_ENDPOINT=$(terraform output -raw rds_address 2>/dev/null || echo "")
          DB_NAME=$(terraform output -raw rds_database_name 2>/dev/null || echo "devopsdb")
          DB_USER=$(terraform output -raw rds_username 2>/dev/null || echo "dbadmin")
          
          # Check if we have EC2 IPs
          if [ -z "$EC2_IPS" ] || [ "$EC2_IPS" = "null" ]; then
            echo "âš ï¸  No EC2 instances found. Inventory will be empty."
            EC2_HOSTS=""
          else
            EC2_HOSTS=$(echo "$EC2_IPS" | awk '{print "ec2-"NR" ansible_host="$1}')
          fi
          
          # Generate inventory file
          cat > ../ansible/inventory/hosts.ini <<EOF
          # Auto-generated Ansible Inventory
          # Generated on: $(date)
          
          [ec2_instances]
          $EC2_HOSTS
          
          [ec2_instances:vars]
          ansible_user=ec2-user
          ansible_ssh_private_key_file=~/.ssh/id_rsa
          ansible_python_interpreter=/usr/bin/python3
          
          [app_servers]
          $EC2_HOSTS
          
          [app_servers:vars]
          app_name=devops-mid
          app_port=5000
          db_host=$RDS_ENDPOINT
          db_port=5432
          db_name=$DB_NAME
          db_user=$DB_USER
          db_password=${{ secrets.DB_PASSWORD }}
          
          [all:vars]
          ansible_connection=ssh
          ansible_ssh_common_args='-o StrictHostKeyChecking=no'
          EOF
          
          echo "âœ… Inventory file generated"
          if [ -n "$EC2_HOSTS" ]; then
            echo "   EC2 Instances:"
            echo "$EC2_HOSTS" | sed 's/^/     /'
          else
            echo "   âš ï¸  No EC2 instances in inventory"
          fi

      - name: Check if inventory has hosts
        id: check_inventory
        run: |
          cd ansible
          if grep -q "ansible_host=" inventory/hosts.ini; then
            echo "has_hosts=true" >> $GITHUB_OUTPUT
            echo "âœ… Inventory contains hosts"
          else
            echo "has_hosts=false" >> $GITHUB_OUTPUT
            echo "âš ï¸  Inventory is empty - skipping Ansible steps"
          fi

      - name: Test Ansible Connectivity
        if: steps.check_inventory.outputs.has_hosts == 'true'
        run: |
          cd ansible
          ansible all -m ping -i inventory/hosts.ini
        continue-on-error: true

      - name: Run Ansible Playbook
        if: steps.check_inventory.outputs.has_hosts == 'true'
        run: |
          cd ansible
          ansible-playbook -i inventory/hosts.ini playbook.yaml -v \
            -e "docker_image=${{ secrets.DOCKERHUB_USERNAME }}/devops-mid:latest" \
            -e "dockerhub_username=${{ secrets.DOCKERHUB_USERNAME }}" \
            -e "dockerhub_token=${{ secrets.DOCKERHUB_TOKEN }}" \
            -e "deploy_app=true"

      - name: Verify Installation
        if: steps.check_inventory.outputs.has_hosts == 'true'
        run: |
          cd ansible
          ansible ec2_instances -i inventory/hosts.ini -a "docker --version"
          ansible ec2_instances -i inventory/hosts.ini -a "systemctl status docker" -b
        continue-on-error: true

  # ============================================
  # STAGE 6: Post-Deployment Smoke Tests
  # ============================================
  smoke-tests:
    name: "âœ… Stage 6: Smoke Tests"
    runs-on: ubuntu-latest
    needs: [terraform-provision, ansible-deploy]
    if: github.event_name != 'pull_request'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install test dependencies
        run: |
          pip install requests pytest boto3

      - name: Wait for services to stabilize
        run: sleep 30

      - name: Run smoke tests
        run: |
          python scripts/smoke_tests.py
        env:
          EC2_IPS: ${{ needs.terraform-provision.outputs.ec2_ips }}
          RDS_ENDPOINT: ${{ needs.terraform-provision.outputs.rds_endpoint }}
        continue-on-error: true

      - name: Health Check - EC2 Instances
        run: |
          EC2_IPS='${{ needs.terraform-provision.outputs.ec2_ips }}'
          for ip in $(echo $EC2_IPS | jq -r '.[]'); do
            echo "Checking EC2: $ip"
            curl -f http://$ip:5000/health || echo "Health check failed for $ip"
          done
        continue-on-error: true

      - name: Upload smoke test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: smoke-test-results
          path: |
            smoke-test-*.log
          retention-days: 7

  # ============================================
  # STAGE 7: Infrastructure Teardown
  # ============================================
  terraform-destroy:
    name: "ðŸ’¥ Stage 7: Terraform Destroy"
    runs-on: ubuntu-latest
    needs: [terraform-provision, ansible-deploy, smoke-tests]
    if: |
      always() && 
      github.event_name != 'pull_request' && 
      !inputs.skip_destroy &&
      needs.terraform-provision.result == 'success'
    defaults:
      run:
        working-directory: infra

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Download Terraform State
        uses: actions/download-artifact@v4
        with:
          name: terraform-state
          path: infra/
        continue-on-error: true

      - name: Terraform Init
        run: terraform init

      - name: Wait before destroy (allow time for testing)
        run: sleep 120

      - name: Terraform Destroy
        run: |
          terraform destroy -auto-approve \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="docker_image=${{ secrets.DOCKERHUB_USERNAME }}/devops-mid:latest" \
            -var="ec2_instance_type=t3.micro" \
            -var="ec2_instance_count=1" \
            -var="db_instance_class=db.t3.micro"

      - name: Cleanup Orphaned Resources
        run: |
          # Clean up any orphaned DB subnet groups
          for sg in $(aws rds describe-db-subnet-groups --query "DBSubnetGroups[?starts_with(DBSubnetGroupName, '${{ env.PROJECT_NAME }}-db-subnet')].DBSubnetGroupName" --output text 2>/dev/null); do
            echo "Deleting orphaned DB subnet group: $sg"
            aws rds delete-db-subnet-group --db-subnet-group-name "$sg" 2>/dev/null || true
          done

          # Clean up any orphaned DB instances
          for db in $(aws rds describe-db-instances --query "DBInstances[?starts_with(DBInstanceIdentifier, '${{ env.PROJECT_NAME }}-postgres')].DBInstanceIdentifier" --output text 2>/dev/null); do
            echo "Deleting orphaned DB instance: $db"
            aws rds delete-db-instance --db-instance-identifier "$db" --skip-final-snapshot --delete-automated-backups 2>/dev/null || true
          done
        continue-on-error: true

      - name: Verify Cleanup
        run: |
          aws ec2 describe-instances \
            --filters "Name=tag:Project,Values=devops-mid" "Name=instance-state-name,Values=running" \
            --query 'Reservations[].Instances[].InstanceId' || echo "No instances found"

          aws rds describe-db-instances \
            --query 'DBInstances[?DBInstanceIdentifier==`devops-mid-postgres`].DBInstanceIdentifier' || echo "No RDS found"

  # ============================================
  # Final Status Report
  # ============================================
  pipeline-status:
    name: "ðŸ“Š Pipeline Status Report"
    runs-on: ubuntu-latest
    needs:
      [
        build-and-test,
        security-and-lint,
        docker-build-push,
        terraform-provision,
        ansible-deploy,
        smoke-tests,
        terraform-destroy,
      ]
    if: always()

    steps:
      - name: Generate Status Report
        run: |
          echo "# CI/CD Pipeline Status Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Build & Test | ${{ needs.build-and-test.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security & Lint | ${{ needs.security-and-lint.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Docker Build | ${{ needs.docker-build-push.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Terraform Provision | ${{ needs.terraform-provision.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Ansible Deploy | ${{ needs.ansible-deploy.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Smoke Tests | ${{ needs.smoke-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Terraform Destroy | ${{ needs.terraform-destroy.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Deployment Info" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Triggered by:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Workflow run:** https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY

      - name: Check if pipeline failed
        if: |
          needs.build-and-test.result == 'failure' ||
          needs.security-and-lint.result == 'failure' ||
          needs.docker-build-push.result == 'failure' ||
          needs.terraform-provision.result == 'failure' ||
          needs.ansible-deploy.result == 'failure'
        run: |
          echo "::error::Pipeline failed at one or more stages"
          exit 1
