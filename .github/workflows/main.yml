name: Complete CI/CD Pipeline - Deploy & Destroy

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      skip_destroy:
        description: "Skip infrastructure destroy at the end"
        required: false
        type: boolean
        default: false
      deployment_mode:
        description: "Deployment mode"
        required: false
        type: choice
        default: "full"
        options:
          - full
          - infrastructure-only
          - configuration-only

permissions:
  contents: read
  packages: write
  id-token: write

env:
  PYTHON_VERSION: "3.11"
  TERRAFORM_VERSION: "1.6.0"
  ANSIBLE_VERSION: "8.7.0"
  AWS_REGION: "us-east-1"
  PROJECT_NAME: "devops-mid"

jobs:
  # ============================================
  # STAGE 1: Build & Test Application
  # ============================================
  build-and-test:
    name: "ðŸ“¦ Stage 1: Build & Test"
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run unit tests
        run: |
          pytest tests/ -v --cov=app --cov-report=xml --cov-report=term
        continue-on-error: true

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results
          path: |
            coverage.xml
            pytest-results.xml
          retention-days: 7

      - name: Test database connection
        run: |
          python test_db_connection.py || echo "Database test skipped (no DB yet)"
        continue-on-error: true

  # ============================================
  # STAGE 2: Security Scanning & Linting
  # ============================================
  security-and-lint:
    name: "ðŸ”’ Stage 2: Security & Linting"
    runs-on: ubuntu-latest
    needs: build-and-test

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install linting tools
        run: |
          pip install flake8 pylint bandit safety black isort

      - name: Run Flake8
        run: |
          flake8 app/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 app/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
        continue-on-error: true

      - name: Run Pylint
        run: |
          pylint app/ --exit-zero
        continue-on-error: true

      - name: Run Bandit (Security Scanner)
        run: |
          bandit -r app/ -f json -o bandit-report.json || true
          bandit -r app/ -ll
        continue-on-error: true

      - name: Check Python dependencies for vulnerabilities
        run: |
          safety check --json || true
        continue-on-error: true

      - name: Run Black (Code Formatter Check)
        run: |
          black --check app/ || true
        continue-on-error: true

      - name: Run isort (Import Sorter Check)
        run: |
          isort --check-only app/ || true
        continue-on-error: true

      - name: Terraform Security Scan (tfsec)
        uses: aquasecurity/tfsec-action@v1.0.3
        with:
          working_directory: infra
          soft_fail: true

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
          retention-days: 30

  # ============================================
  # STAGE 3: Docker Build & Push
  # ============================================
  docker-build-push:
    name: "ðŸ³ Stage 3: Docker Build & Push"
    runs-on: ubuntu-latest
    needs: security-and-lint
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Log in to GitHub Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: |
            ${{ secrets.DOCKERHUB_USERNAME }}/devops-mid
            ghcr.io/${{ github.repository }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ secrets.DOCKERHUB_USERNAME }}/devops-mid:latest
          format: "sarif"
          output: "trivy-results.sarif"
        continue-on-error: true

      - name: Upload Trivy results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: "trivy-results.sarif"
        continue-on-error: true

  # ============================================
  # STAGE 4: Terraform Infrastructure Provisioning
  # ============================================
  terraform-provision:
    name: "ðŸ—ï¸ Stage 4: Terraform Provision"
    runs-on: ubuntu-latest
    needs: docker-build-push
    if: github.event_name != 'pull_request'
    defaults:
      run:
        working-directory: infra
    outputs:
      ec2_ips: ${{ steps.output.outputs.ec2_ips }}
      rds_address: ${{ steps.output.outputs.rds_address }}
      rds_endpoint: ${{ steps.output.outputs.rds_endpoint }}
      db_name: ${{ steps.output.outputs.db_name }}
      db_user: ${{ steps.output.outputs.db_user }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      - name: Terraform Format Check
        run: terraform fmt -check -recursive
        continue-on-error: true

      - name: Terraform Init
        run: terraform init

      - name: Terraform Validate
        run: terraform validate

      - name: Clean Up Conflicting Resources
        run: |
          # Configure AWS CLI
          export AWS_DEFAULT_REGION=${{ env.AWS_REGION }}

          # ==== CLEAN UP OLD VPCs (to avoid VpcLimitExceeded) ====
          echo "Checking for old project VPCs..."
          for vpc in $(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" --query "Vpcs[*].VpcId" --output text 2>/dev/null); do
            echo "Found VPC: $vpc - cleaning up dependencies..."
            
            # Delete EC2 instances in VPC
            for instance in $(aws ec2 describe-instances --filters "Name=vpc-id,Values=$vpc" "Name=instance-state-name,Values=running,stopped" --query "Reservations[*].Instances[*].InstanceId" --output text 2>/dev/null); do
              echo "Terminating EC2 instance: $instance"
              aws ec2 terminate-instances --instance-ids "$instance" 2>/dev/null || true
            done
            
            # Wait for instances to terminate
            sleep 30
            
            # Delete NAT Gateways
            for nat in $(aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$vpc" "Name=state,Values=available" --query "NatGateways[*].NatGatewayId" --output text 2>/dev/null); do
              echo "Deleting NAT Gateway: $nat"
              aws ec2 delete-nat-gateway --nat-gateway-id "$nat" 2>/dev/null || true
            done
            
            # Delete Internet Gateways
            for igw in $(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$vpc" --query "InternetGateways[*].InternetGatewayId" --output text 2>/dev/null); do
              echo "Detaching and deleting Internet Gateway: $igw"
              aws ec2 detach-internet-gateway --internet-gateway-id "$igw" --vpc-id "$vpc" 2>/dev/null || true
              aws ec2 delete-internet-gateway --internet-gateway-id "$igw" 2>/dev/null || true
            done
            
            # Delete Subnets
            for subnet in $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc" --query "Subnets[*].SubnetId" --output text 2>/dev/null); do
              echo "Deleting Subnet: $subnet"
              aws ec2 delete-subnet --subnet-id "$subnet" 2>/dev/null || true
            done
            
            # Delete Route Tables (non-main)
            for rt in $(aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$vpc" --query "RouteTables[?Associations[0].Main!=\`true\`].RouteTableId" --output text 2>/dev/null); do
              echo "Deleting Route Table: $rt"
              aws ec2 delete-route-table --route-table-id "$rt" 2>/dev/null || true
            done
            
            # Delete Security Groups (non-default)
            for sg in $(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$vpc" --query "SecurityGroups[?GroupName!=\`default\`].GroupId" --output text 2>/dev/null); do
              echo "Deleting Security Group: $sg"
              aws ec2 delete-security-group --group-id "$sg" 2>/dev/null || true
            done
            
            # Finally delete the VPC
            echo "Deleting VPC: $vpc"
            aws ec2 delete-vpc --vpc-id "$vpc" 2>/dev/null || echo "Could not delete VPC $vpc - may have remaining dependencies"
          done

          # ==== Delete orphaned DB instances FIRST (before subnet groups) ====
          echo "Checking for existing DB instances..."
          DB_INSTANCES=$(aws rds describe-db-instances --query "DBInstances[?starts_with(DBInstanceIdentifier, '${{ env.PROJECT_NAME }}-postgres')].DBInstanceIdentifier" --output text 2>/dev/null || echo "")
          
          if [ -n "$DB_INSTANCES" ]; then
            for db in $DB_INSTANCES; do
              echo "Found DB instance: $db - checking status..."
              status=$(aws rds describe-db-instances --db-instance-identifier "$db" --query "DBInstances[0].DBInstanceStatus" --output text 2>/dev/null || echo "not-found")
              
              if [ "$status" != "deleting" ] && [ "$status" != "not-found" ]; then
                echo "Deleting DB instance: $db (this may take a few minutes)..."
                aws rds delete-db-instance --db-instance-identifier "$db" --skip-final-snapshot --delete-automated-backups 2>/dev/null || echo "Could not delete $db"
              elif [ "$status" = "deleting" ]; then
                echo "DB instance $db is already being deleted, waiting..."
              fi
            done
            
            # Wait for DB instances to be deleted (they can take 5-10 minutes)
            echo "Waiting for DB instances to be deleted..."
            MAX_WAIT=600  # 10 minutes max wait
            ELAPSED=0
            while [ $ELAPSED -lt $MAX_WAIT ]; do
              REMAINING=$(aws rds describe-db-instances --query "DBInstances[?starts_with(DBInstanceIdentifier, '${{ env.PROJECT_NAME }}-postgres')].DBInstanceIdentifier" --output text 2>/dev/null || echo "")
              if [ -z "$REMAINING" ]; then
                echo "All DB instances deleted successfully"
                break
              fi
              echo "Still waiting for DB instances to be deleted... ($ELAPSED seconds elapsed)"
              sleep 30
              ELAPSED=$((ELAPSED + 30))
            done
            
            if [ $ELAPSED -ge $MAX_WAIT ]; then
              echo "âš ï¸  Warning: DB instances may still be deleting, but proceeding with cleanup"
            fi
          else
            echo "No existing DB instances found"
          fi

          # ==== Delete DB subnet groups AFTER DB instances are gone ====
          echo "Deleting DB subnet groups..."
          DB_SUBNET_GROUPS=$(aws rds describe-db-subnet-groups --query "DBSubnetGroups[?starts_with(DBSubnetGroupName, '${{ env.PROJECT_NAME }}-db-subnet')].DBSubnetGroupName" --output text 2>/dev/null || echo "")
          
          if [ -n "$DB_SUBNET_GROUPS" ]; then
            for sg in $DB_SUBNET_GROUPS; do
              echo "Deleting DB subnet group: $sg"
              # Try to delete, but don't fail if it's still in use
              aws rds delete-db-subnet-group --db-subnet-group-name "$sg" 2>/dev/null || {
                echo "Could not delete $sg immediately (may still be in use), will retry..."
                # Force delete attempt after a short wait
                sleep 10
                aws rds delete-db-subnet-group --db-subnet-group-name "$sg" 2>/dev/null || echo "âš ï¸  Could not delete $sg - will be handled by Terraform"
              }
            done
            sleep 10  # Give AWS time to process the deletion
          else
            echo "No existing DB subnet groups found"
          fi

          echo "Resource cleanup completed"
        continue-on-error: true

      - name: Import Existing Resources (if any)
        run: |
          set -euo pipefail

          # Import existing key pair if present. Some provider import implementations may try
          # to call ImportKeyPair which fails if the key already exists; to avoid that,
          # temporarily comment out the public_key line in the resource before importing.
          if aws ec2 describe-key-pairs --query "KeyPairs[?KeyName=='${{ env.PROJECT_NAME }}-key'].KeyName" --output text 2>/dev/null | grep -q .; then
            echo "Existing key pair '${{ env.PROJECT_NAME }}-key' detected. Preparing to import into Terraform state."
            EC2_TF=ec2.tf
            BACKUP=${EC2_TF}.bak
            if [ -f "$EC2_TF" ]; then
              cp "$EC2_TF" "$BACKUP"
              # Comment out the public_key line to avoid provider trying to import key material
              sed -E "s/^(\s*public_key\s*=\s*.*)/# \1/" "$BACKUP" > "$EC2_TF"
            fi
            terraform import aws_key_pair.deployer "${{ env.PROJECT_NAME }}-key" || echo "Import failed or already imported"
            # Restore original file if we modified it
            if [ -f "$BACKUP" ]; then
              mv "$BACKUP" "$EC2_TF"
            fi
          else
            echo "No existing key pair named '${{ env.PROJECT_NAME }}-key'"
          fi

          # Note: We don't import DB subnet groups - we delete them in cleanup step
          # This ensures we start fresh with the correct VPC/subnets
          echo "Skipping DB subnet group import - will be created fresh by Terraform"

          # Detect and import existing RDS DB instances matching the project prefix
          DB_INSTANCES=$(aws rds describe-db-instances --query "DBInstances[?starts_with(DBInstanceIdentifier, '${{ env.PROJECT_NAME }}-postgres')].DBInstanceIdentifier" --output text 2>/dev/null || true)
          if [ -n "$DB_INSTANCES" ]; then
            for db in $DB_INSTANCES; do
              echo "Found existing RDS instance: $db - importing into Terraform state as aws_db_instance.postgres"
              terraform import aws_db_instance.postgres "$db" || echo "Failed to import $db"
              break
            done
          else
            echo "No existing RDS instances found with prefix '${{ env.PROJECT_NAME }}-postgres'"
          fi
        continue-on-error: true

      - name: Terraform Plan
        run: |
          terraform plan \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="docker_image=${{ secrets.DOCKERHUB_USERNAME }}/devops-mid:latest" \
            -var="ec2_instance_type=t3.micro" \
            -var="ec2_instance_count=1" \
            -var="manage_ssh_key=false" \
            -var="db_instance_class=db.t3.micro" \
            -out=tfplan

      - name: Final Cleanup - Ensure DB Subnet Groups are Deleted
        run: |
          echo "Final check: Ensuring DB subnet groups are deleted before Terraform apply..."
          
          # Check for existing DB subnet groups
          DB_SUBNET_GROUPS=$(aws rds describe-db-subnet-groups --query "DBSubnetGroups[?starts_with(DBSubnetGroupName, '${{ env.PROJECT_NAME }}-db-subnet')].DBSubnetGroupName" --output text 2>/dev/null || echo "")
          
          if [ -n "$DB_SUBNET_GROUPS" ]; then
            echo "âš ï¸  Found existing DB subnet groups: $DB_SUBNET_GROUPS"
            
            for sg in $DB_SUBNET_GROUPS; do
              echo "Processing DB subnet group: $sg"
              
              # Check if any DB instances are using this subnet group
              DB_USING_SG=$(aws rds describe-db-instances --query "DBInstances[?DBSubnetGroup.DBSubnetGroupName=='$sg' && DBInstanceStatus!='deleting'].DBInstanceIdentifier" --output text 2>/dev/null || echo "")
              
              if [ -n "$DB_USING_SG" ]; then
                echo "âš ï¸  DB subnet group $sg is in use by DB instances: $DB_USING_SG"
                echo "   Deleting DB instances first..."
                for db in $DB_USING_SG; do
                  echo "   Deleting DB instance: $db"
                  aws rds delete-db-instance --db-instance-identifier "$db" --skip-final-snapshot --delete-automated-backups 2>/dev/null || true
                done
                echo "   Waiting 60 seconds for DB deletion to start..."
                sleep 60
              fi
              
              # Try to delete the subnet group with retries
              MAX_RETRIES=3
              RETRY_COUNT=0
              DELETED=false
              
              while [ $RETRY_COUNT -lt $MAX_RETRIES ] && [ "$DELETED" = false ]; do
                echo "   Attempt $((RETRY_COUNT + 1))/$MAX_RETRIES: Deleting DB subnet group $sg..."
                
                if aws rds delete-db-subnet-group --db-subnet-group-name "$sg" 2>/dev/null; then
                  echo "   âœ… Successfully deleted $sg"
                  DELETED=true
                  sleep 10  # Wait for AWS to process the deletion
                else
                  RETRY_COUNT=$((RETRY_COUNT + 1))
                  if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                    echo "   âš ï¸  Deletion failed, waiting 15 seconds before retry..."
                    sleep 15
                  else
                    echo "   âŒ Failed to delete $sg after $MAX_RETRIES attempts"
                    echo "   Attempting to import into Terraform state instead..."
                    terraform import aws_db_subnet_group.main "$sg" 2>/dev/null && {
                      echo "   âœ… Successfully imported $sg into Terraform state"
                    } || {
                      echo "   âŒ Failed to import $sg - Terraform apply may fail"
                    }
                  fi
                fi
              done
            done
            
            # Final verification
            sleep 5
            REMAINING=$(aws rds describe-db-subnet-groups --query "DBSubnetGroups[?starts_with(DBSubnetGroupName, '${{ env.PROJECT_NAME }}-db-subnet')].DBSubnetGroupName" --output text 2>/dev/null || echo "")
            if [ -n "$REMAINING" ]; then
              echo "âš ï¸  Warning: Some DB subnet groups still exist: $REMAINING"
              echo "   Attempting to import remaining groups into Terraform state..."
              for sg in $REMAINING; do
                terraform import aws_db_subnet_group.main "$sg" 2>/dev/null || echo "   Failed to import $sg"
              done
            else
              echo "âœ… All DB subnet groups successfully deleted"
            fi
          else
            echo "âœ… No existing DB subnet groups found"
          fi

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan

      - name: Save Terraform State
        uses: actions/upload-artifact@v4
        with:
          name: terraform-state
          path: infra/terraform.tfstate
          retention-days: 7

      - name: Extract Terraform Outputs
        id: output
        run: |
          set -euo pipefail

          # Safely capture ec2_public_ips as JSON array or default to empty array
          EC2_JSON=$(terraform output -json ec2_public_ips 2>/dev/null || echo '[]')
          # Normalize Terraform warning text if present
          if echo "$EC2_JSON" | grep -qi "no outputs"; then
            EC2_JSON='[]'
          fi
          echo "ec2_ips=$EC2_JSON" >> $GITHUB_OUTPUT

          # Capture RDS address (not endpoint, as endpoint includes port)
          RDS_ADDR=$(terraform output -raw rds_address 2>/dev/null || echo '')
          if echo "$RDS_ADDR" | grep -qi "no outputs"; then
            RDS_ADDR=''
          fi
          echo "rds_address=$RDS_ADDR" >> $GITHUB_OUTPUT

          # Also capture RDS endpoint for reference
          RDS_EP=$(terraform output -raw rds_endpoint 2>/dev/null || echo '')
          if echo "$RDS_EP" | grep -qi "no outputs"; then
            RDS_EP=''
          fi
          echo "rds_endpoint=$RDS_EP" >> $GITHUB_OUTPUT

          # Capture DB name and username
          DB_NAME=$(terraform output -raw rds_database_name 2>/dev/null || echo 'devopsdb')
          if echo "$DB_NAME" | grep -qi "no outputs"; then
            DB_NAME='devopsdb'
          fi
          echo "db_name=$DB_NAME" >> $GITHUB_OUTPUT

          DB_USER=$(terraform output -raw rds_username 2>/dev/null || echo 'dbadmin')
          if echo "$DB_USER" | grep -qi "no outputs"; then
            DB_USER='dbadmin'
          fi
          echo "db_user=$DB_USER" >> $GITHUB_OUTPUT

          # Persist human-readable terraform output but do not fail if absent
          mkdir -p ${{ github.workspace }}/outputs
          terraform output > ${{ github.workspace }}/outputs/terraform-output.txt 2>/dev/null || true
          
          echo "âœ… Extracted Terraform outputs:"
          echo "   EC2 IPs: $EC2_JSON"
          echo "   RDS Address: $RDS_ADDR"
          echo "   DB Name: $DB_NAME"
          echo "   DB User: $DB_USER"

      - name: Upload Terraform Outputs
        uses: actions/upload-artifact@v4
        with:
          name: terraform-outputs
          path: ${{ github.workspace }}/outputs/terraform-output.txt
          retention-days: 30

      - name: Wait for EC2 to be ready
        run: sleep 60

  # ============================================
  # STAGE 5: Ansible Configuration & Deployment
  # ============================================
  ansible-deploy:
    name: "âš™ï¸ Stage 5: Ansible Deploy"
    runs-on: ubuntu-latest
    needs: terraform-provision
    if: github.event_name != 'pull_request'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Ansible and dependencies
        run: |
          pip install ansible==${{ env.ANSIBLE_VERSION }}
          pip install boto3 botocore
          cd ansible
          ansible-galaxy collection install -r requirements.yml

      - name: Configure AWS credentials for dynamic inventory
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          
          # Extract EC2 IPs from JSON and add to known_hosts
          EC2_IPS_JSON='${{ needs.terraform-provision.outputs.ec2_ips }}'
          if [ -n "$EC2_IPS_JSON" ] && [ "$EC2_IPS_JSON" != "[]" ] && [ "$EC2_IPS_JSON" != "null" ]; then
            echo "$EC2_IPS_JSON" | jq -r '.[]' | while read ip; do
              if [ -n "$ip" ]; then
                echo "Adding $ip to known_hosts..."
                ssh-keyscan -H "$ip" >> ~/.ssh/known_hosts 2>/dev/null || true
              fi
            done
          fi

      - name: Setup Terraform for inventory generation
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      - name: Install jq
        run: |
          sudo apt-get update && sudo apt-get install -y jq

      - name: Download Terraform State
        uses: actions/download-artifact@v4
        with:
          name: terraform-state
          path: infra/
        continue-on-error: true

      - name: Generate Ansible Inventory from Terraform Outputs
        run: |
          # Use outputs from terraform-provision job (already extracted)
          EC2_IPS_JSON='${{ needs.terraform-provision.outputs.ec2_ips }}'
          RDS_ADDRESS='${{ needs.terraform-provision.outputs.rds_address }}'
          DB_NAME='${{ needs.terraform-provision.outputs.db_name }}'
          DB_USER='${{ needs.terraform-provision.outputs.db_user }}'
          
          echo "Generating Ansible inventory from Terraform outputs..."
          echo "EC2 IPs JSON: $EC2_IPS_JSON"
          echo "RDS Address: $RDS_ADDRESS"
          echo "DB Name: $DB_NAME"
          echo "DB User: $DB_USER"
          
          # Parse EC2 IPs from JSON array
          if [ -n "$EC2_IPS_JSON" ] && [ "$EC2_IPS_JSON" != "[]" ] && [ "$EC2_IPS_JSON" != "null" ]; then
            # Extract IPs from JSON array using jq
            EC2_IPS=$(echo "$EC2_IPS_JSON" | jq -r '.[]' 2>/dev/null || echo "")
            
            if [ -n "$EC2_IPS" ]; then
              # Generate host entries
              EC2_HOSTS=$(echo "$EC2_IPS" | awk '{print "ec2-"NR" ansible_host="$1}')
              echo "âœ… Found EC2 instances:"
              echo "$EC2_HOSTS" | sed 's/^/   /'
            else
              EC2_HOSTS=""
              echo "âš ï¸  No EC2 IPs found in JSON"
            fi
          else
            EC2_HOSTS=""
            echo "âš ï¸  No EC2 instances found in Terraform outputs"
          fi
          
          # Use RDS address directly (already extracted from Terraform)
          if [ -z "$RDS_ADDRESS" ] || [ "$RDS_ADDRESS" = "null" ]; then
            RDS_ADDRESS=""
            echo "âš ï¸  No RDS address found"
          else
            echo "âœ… RDS Address: $RDS_ADDRESS"
          fi
          
          # Use DB credentials from outputs (with defaults)
          if [ -z "$DB_NAME" ] || [ "$DB_NAME" = "null" ]; then
            DB_NAME="devopsdb"
          fi
          if [ -z "$DB_USER" ] || [ "$DB_USER" = "null" ]; then
            DB_USER="dbadmin"
          fi
          
          # Generate inventory file
          cat > ansible/inventory/hosts.ini <<EOF
          # Auto-generated Ansible Inventory
          # Generated on: $(date)
          # Source: Terraform outputs from terraform-provision job
          
          [ec2_instances]
          $EC2_HOSTS
          
          [ec2_instances:vars]
          ansible_user=ec2-user
          ansible_ssh_private_key_file=~/.ssh/id_rsa
          ansible_python_interpreter=/usr/bin/python3
          
          [app_servers]
          $EC2_HOSTS
          
          [app_servers:vars]
          app_name=devops-mid
          app_port=5000
          db_host=$RDS_ADDRESS
          db_port=5432
          db_name=$DB_NAME
          db_user=$DB_USER
          db_password=${{ secrets.DB_PASSWORD }}
          
          [all:vars]
          ansible_connection=ssh
          ansible_ssh_common_args='-o StrictHostKeyChecking=no'
          EOF
          
          echo ""
          echo "âœ… Inventory file generated successfully at ansible/inventory/hosts.ini"
          echo ""
          if [ -n "$EC2_HOSTS" ]; then
            echo "ðŸ“‹ Inventory contents:"
            cat ansible/inventory/hosts.ini
          else
            echo "âš ï¸  Warning: Inventory file is empty (no EC2 instances)"
          fi

      - name: Check if inventory has hosts
        id: check_inventory
        run: |
          cd ansible
          if grep -q "ansible_host=" inventory/hosts.ini; then
            echo "has_hosts=true" >> $GITHUB_OUTPUT
            echo "âœ… Inventory contains hosts"
          else
            echo "has_hosts=false" >> $GITHUB_OUTPUT
            echo "âš ï¸  Inventory is empty - skipping Ansible steps"
          fi

      - name: Test Ansible Connectivity
        if: steps.check_inventory.outputs.has_hosts == 'true'
        run: |
          cd ansible
          ansible all -m ping -i inventory/hosts.ini
        continue-on-error: true

      - name: Run Ansible Playbook
        if: steps.check_inventory.outputs.has_hosts == 'true'
        run: |
          cd ansible
          ansible-playbook -i inventory/hosts.ini playbook.yaml -v \
            -e "docker_image=${{ secrets.DOCKERHUB_USERNAME }}/devops-mid:latest" \
            -e "dockerhub_username=${{ secrets.DOCKERHUB_USERNAME }}" \
            -e "dockerhub_token=${{ secrets.DOCKERHUB_TOKEN }}" \
            -e "deploy_app=true"

      - name: Verify Installation
        if: steps.check_inventory.outputs.has_hosts == 'true'
        run: |
          cd ansible
          ansible ec2_instances -i inventory/hosts.ini -a "docker --version"
          ansible ec2_instances -i inventory/hosts.ini -a "systemctl status docker" -b
        continue-on-error: true

  # ============================================
  # STAGE 6: Post-Deployment Smoke Tests
  # ============================================
  smoke-tests:
    name: "âœ… Stage 6: Smoke Tests"
    runs-on: ubuntu-latest
    needs: [terraform-provision, ansible-deploy]
    if: github.event_name != 'pull_request'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install test dependencies
        run: |
          pip install requests pytest boto3

      - name: Wait for services to stabilize
        run: sleep 30

      - name: Run smoke tests
        run: |
          python scripts/smoke_tests.py
        env:
          EC2_IPS: ${{ needs.terraform-provision.outputs.ec2_ips }}
          RDS_ENDPOINT: ${{ needs.terraform-provision.outputs.rds_endpoint }}
        continue-on-error: true

      - name: Health Check - EC2 Instances
        run: |
          EC2_IPS='${{ needs.terraform-provision.outputs.ec2_ips }}'
          for ip in $(echo $EC2_IPS | jq -r '.[]'); do
            echo "Checking EC2: $ip"
            curl -f http://$ip:5000/health || echo "Health check failed for $ip"
          done
        continue-on-error: true

      - name: Upload smoke test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: smoke-test-results
          path: |
            smoke-test-*.log
          retention-days: 7

  # ============================================
  # STAGE 7: Infrastructure Teardown
  # ============================================
  terraform-destroy:
    name: "ðŸ’¥ Stage 7: Terraform Destroy"
    runs-on: ubuntu-latest
    needs: [terraform-provision, ansible-deploy, smoke-tests]
    if: |
      always() && 
      github.event_name != 'pull_request' && 
      !inputs.skip_destroy &&
      needs.terraform-provision.result == 'success'
    defaults:
      run:
        working-directory: infra

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Download Terraform State
        uses: actions/download-artifact@v4
        with:
          name: terraform-state
          path: infra/
        continue-on-error: true

      - name: Terraform Init
        run: terraform init

      - name: Wait before destroy (allow time for testing)
        run: sleep 120

      - name: Refresh Terraform State
        run: |
          # Refresh state to ensure it's up to date with actual AWS resources
          terraform refresh \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="docker_image=${{ secrets.DOCKERHUB_USERNAME }}/devops-mid:latest" \
            -var="ec2_instance_type=t3.micro" \
            -var="ec2_instance_count=1" \
            -var="db_instance_class=db.t3.micro" || true
        continue-on-error: true

      - name: Force Delete RDS Instance First (to avoid dependency issues)
        run: |
          echo "Checking for RDS instances to delete first..."
          for db in $(aws rds describe-db-instances --query "DBInstances[?starts_with(DBInstanceIdentifier, '${{ env.PROJECT_NAME }}-postgres')].DBInstanceIdentifier" --output text 2>/dev/null); do
            echo "Found RDS instance: $db - initiating deletion..."
            aws rds delete-db-instance --db-instance-identifier "$db" --skip-final-snapshot --delete-automated-backups 2>/dev/null || echo "RDS $db deletion already in progress or failed"
          done
          
          # Wait for RDS to start deleting (don't wait for full completion)
          echo "Waiting 30 seconds for RDS deletion to initiate..."
          sleep 30
        continue-on-error: true

      - name: Terraform Destroy
        run: |
          terraform destroy -auto-approve \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="ssh_public_key=${{ secrets.SSH_PUBLIC_KEY }}" \
            -var="docker_image=${{ secrets.DOCKERHUB_USERNAME }}/devops-mid:latest" \
            -var="ec2_instance_type=t3.micro" \
            -var="ec2_instance_count=1" \
            -var="db_instance_class=db.t3.micro" || echo "Terraform destroy had errors, will attempt manual cleanup"
        continue-on-error: true

      - name: Force Cleanup - Delete All Project Resources
        run: |
          echo "=== Force Cleanup: Deleting all project resources ==="
          
          # 1. Delete RDS instances
          echo "Step 1: Deleting RDS instances..."
          for db in $(aws rds describe-db-instances --query "DBInstances[?starts_with(DBInstanceIdentifier, '${{ env.PROJECT_NAME }}-postgres')].DBInstanceIdentifier" --output text 2>/dev/null); do
            echo "Deleting RDS: $db"
            aws rds delete-db-instance --db-instance-identifier "$db" --skip-final-snapshot --delete-automated-backups 2>/dev/null || true
          done
          
          # 2. Wait for RDS to be deleted or in deleting state
          echo "Step 2: Waiting for RDS instances to delete (max 5 minutes)..."
          for i in {1..30}; do
            REMAINING=$(aws rds describe-db-instances --query "DBInstances[?starts_with(DBInstanceIdentifier, '${{ env.PROJECT_NAME }}-postgres') && DBInstanceStatus!='deleting'].DBInstanceIdentifier" --output text 2>/dev/null)
            if [ -z "$REMAINING" ]; then
              echo "All RDS instances deleted or in deleting state"
              break
            fi
            echo "Attempt $i/30: Still waiting for RDS deletion..."
            sleep 10
          done
          
          # 3. Delete EC2 instances
          echo "Step 3: Terminating EC2 instances..."
          INSTANCES=$(aws ec2 describe-instances --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" "Name=instance-state-name,Values=running,stopped,pending" --query "Reservations[*].Instances[*].InstanceId" --output text 2>/dev/null)
          if [ -n "$INSTANCES" ]; then
            echo "Terminating instances: $INSTANCES"
            aws ec2 terminate-instances --instance-ids $INSTANCES 2>/dev/null || true
            echo "Waiting 60 seconds for instance termination..."
            sleep 60
          fi
          
          # 4. Delete DB subnet groups (after RDS is gone)
          echo "Step 4: Deleting DB subnet groups..."
          for sg in $(aws rds describe-db-subnet-groups --query "DBSubnetGroups[?starts_with(DBSubnetGroupName, '${{ env.PROJECT_NAME }}-db-subnet')].DBSubnetGroupName" --output text 2>/dev/null); do
            echo "Deleting DB subnet group: $sg"
            aws rds delete-db-subnet-group --db-subnet-group-name "$sg" 2>/dev/null || true
          done
          
          # 5. Delete security groups
          echo "Step 5: Deleting security groups..."
          for vpc in $(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" --query "Vpcs[*].VpcId" --output text 2>/dev/null); do
            for sg in $(aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$vpc" --query "SecurityGroups[?GroupName!='default'].GroupId" --output text 2>/dev/null); do
              echo "Deleting security group: $sg"
              aws ec2 delete-security-group --group-id "$sg" 2>/dev/null || true
            done
          done
          
          # 6. Delete subnets, route tables, internet gateways, and VPCs
          echo "Step 6: Cleaning up VPC resources..."
          for vpc in $(aws ec2 describe-vpcs --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" --query "Vpcs[*].VpcId" --output text 2>/dev/null); do
            echo "Cleaning up VPC: $vpc"
            
            # Delete subnets
            for subnet in $(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$vpc" --query "Subnets[*].SubnetId" --output text 2>/dev/null); do
              echo "  Deleting subnet: $subnet"
              aws ec2 delete-subnet --subnet-id "$subnet" 2>/dev/null || true
            done
            
            # Delete route tables (non-main)
            for rt in $(aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$vpc" --query "RouteTables[?Associations[0].Main!=\`true\`].RouteTableId" --output text 2>/dev/null); do
              echo "  Deleting route table: $rt"
              aws ec2 delete-route-table --route-table-id "$rt" 2>/dev/null || true
            done
            
            # Detach and delete internet gateways
            for igw in $(aws ec2 describe-internet-gateways --filters "Name=attachment.vpc-id,Values=$vpc" --query "InternetGateways[*].InternetGatewayId" --output text 2>/dev/null); do
              echo "  Detaching and deleting IGW: $igw"
              aws ec2 detach-internet-gateway --internet-gateway-id "$igw" --vpc-id "$vpc" 2>/dev/null || true
              aws ec2 delete-internet-gateway --internet-gateway-id "$igw" 2>/dev/null || true
            done
            
            # Delete VPC
            echo "  Deleting VPC: $vpc"
            aws ec2 delete-vpc --vpc-id "$vpc" 2>/dev/null || echo "Could not delete VPC $vpc - may have remaining dependencies"
          done
          
          # 7. Delete key pairs
          echo "Step 7: Deleting key pairs..."
          aws ec2 delete-key-pair --key-name "${{ env.PROJECT_NAME }}-key" 2>/dev/null || true
          
          echo "=== Force cleanup completed ==="

      - name: Verify Cleanup
        run: |
          echo "=== Verifying cleanup ==="
          
          echo "Checking for remaining EC2 instances..."
          aws ec2 describe-instances \
            --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" "Name=instance-state-name,Values=running,stopped,pending" \
            --query 'Reservations[].Instances[].[InstanceId,State.Name]' --output table || echo "No EC2 instances found"

          echo "Checking for remaining RDS instances..."
          aws rds describe-db-instances \
            --query "DBInstances[?starts_with(DBInstanceIdentifier, '${{ env.PROJECT_NAME }}-postgres')].[DBInstanceIdentifier,DBInstanceStatus]" --output table || echo "No RDS instances found"

          echo "Checking for remaining VPCs..."
          aws ec2 describe-vpcs \
            --filters "Name=tag:Project,Values=${{ env.PROJECT_NAME }}" \
            --query 'Vpcs[].[VpcId,State]' --output table || echo "No VPCs found"
            
          echo "=== Verification complete ==="

  # ============================================
  # Final Status Report
  # ============================================
  pipeline-status:
    name: "ðŸ“Š Pipeline Status Report"
    runs-on: ubuntu-latest
    needs:
      [
        build-and-test,
        security-and-lint,
        docker-build-push,
        terraform-provision,
        ansible-deploy,
        smoke-tests,
        terraform-destroy,
      ]
    if: always()

    steps:
      - name: Generate Status Report
        run: |
          echo "# CI/CD Pipeline Status Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Build & Test | ${{ needs.build-and-test.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Security & Lint | ${{ needs.security-and-lint.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Docker Build | ${{ needs.docker-build-push.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Terraform Provision | ${{ needs.terraform-provision.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Ansible Deploy | ${{ needs.ansible-deploy.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Smoke Tests | ${{ needs.smoke-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Terraform Destroy | ${{ needs.terraform-destroy.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Deployment Info" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Triggered by:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Workflow run:** https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY

      - name: Check if pipeline failed
        if: |
          needs.build-and-test.result == 'failure' ||
          needs.security-and-lint.result == 'failure' ||
          needs.docker-build-push.result == 'failure' ||
          needs.terraform-provision.result == 'failure' ||
          needs.ansible-deploy.result == 'failure'
        run: |
          echo "::error::Pipeline failed at one or more stages"
          exit 1
